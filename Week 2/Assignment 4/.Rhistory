a[1]
b <- a(c[1:4])
b <- a[c(1,4)]
b
a <- c(1,3,2,5,4,5,7,8,9,6,2,1,4)
b <- a[c(1,4)]
b
d <- a[1:4]
d
d[-1]
d
a>3
a/5
anyvector <- a>3
a[anyvector]
x <- 1:30
x[x>5]
help(rnbinom)
counts <- rnbinom(10000, mu = 0.92, size = 1.1)
counts[1:30]
table(counts)
setA <- c("a", "b", "c", "d", "e")
setB <- c("d", "e", "f", "g")
union(setA,setB)
intersect(setA,setB)
setdiff(setA,setB)
x <- list(1, "c", FALSE)
x
x <- list(col1 = 1:3, col2 = 4)
x
x[1]
x[[1]][2]
x[1]
x[1]
x[[1]]
m1 <- matrix(nrow=4, ncol=5)
m1
dim(m1)
m1 <- matrix(1:10, nrow=2, ncol=5)
m1
dim(m1)
matrix(data = c(1,2,3,4,5), byrow=TRUE, nrow=2)
matrix(data = c(1,2,3,4,5), byrow=TRUE, nrow=2)
matrix(data = c(1,2,3,4,5), byrow=TRUE, nrow=2)
matrix(data = c(1,2,3,4), byrow=TRUE, nrow=2)
x <- matrix(1:10,2,5)
x
x[1,1]
x[1,]
x[,2]
X <- 1:6
y <- 12:17
cbind(x,y)
rbind(x,y)
X <- 1:6
y <- 12:17
cbind(x,y)
rbind(x,y)
x <- data.frame(col1 = 1:20, col2 = c(T,F,F,T))
x
nrow(x)
ncol(x)
str(x) #structure of data frame
x[2:5,1]
#for loop
x <- c("a","b","c")
for(i in seq_along(x))
print(x[i])
for(letter in x)
print(x[letter])
func1 <- function(a,b) {
a+b
}
func1(2+2)
func1(2,2)
square.it <- function(x) {
square <- x*x
return(square)
}
squareit(5)
square.it(5)
setwd("E:/Data-Mining/Week 2")
#----------------------#
#Problem-2
#----------------------#
data = read.csv("sonar_test.csv", header = FALSE)
x = data[, 1:2]
plot(x, pch = 19, xlab = expression(x[1]), ylab = expression(x[2]))
fit = kmeans(x, 2)
points(fit$centers, pch=19, col="blue", cex=2)
library(class)
knnfit<-knn(fit$centers, x, as.factor(c(-1,1)))
points(x, col=1+1*as.numeric(knnfit), pch=19)
#----------------------#
#Problem-3
#----------------------#
plot(x, pch=19, xlab=expression(x[1]), ylab=expression(x[2]))
y<-data[,61]
points(x, col=2+1*y, pch=19)
#misclassification error
1-sum(knnfit==y)/length(y) #0.525641
#----------------------#
#Problem-4
#----------------------#
x<-data[,1:60]
fit<-kmeans(x, 2)
knnfit<-knn(fit$centers, x, as.factor(c(-1,1)))
1-sum(knnfit==y)/length(y) #0.4358
#----------------------#
#Problem-6
#----------------------#
x<-c(1,2,2.5,3,3.5,4,4.5,5,7,8,8.5,9,9.5,10)
center1<-1
center2<-2
for (k in 2:10){
cluster1<-x[abs(x-center1[k-1])<=abs(x-center2[k-1])]
cluster2<-x[abs(x-center1[k-1])>abs(x-center2[k-1])]
center1[k]<-mean(cluster1)
center2[k]<-mean(cluster2)
}
for (k in 2:10){
cluster1<-x[abs(x-center1[k-1])<=abs(x-center2[k-1])]
cluster2<-x[abs(x-center1[k-1])>abs(x-center2[k-1])]
center1[k]<-mean(cluster1)
center2[k]<-mean(cluster2)
}
#----------------------#
#Problem-7
#----------------------#
x<-c(1,2,2.5,3,3.5,4,4.5,5,7,8,8.5,9,9.5,10)
kmeans(x,2)
#----------------------#
#Problem-8
#----------------------#
x1<-c(1,2)
x2<-c(5,10)
dist_vector = rbind(x1, x2)
dist(dist_vector)
#----------------------#
#Problem-9
#----------------------#
x1<-c(1,2,3,6)
x2<-c(5,10,4,12)
#(b)
dist_vector = rbind(x1, x2)
dist(dist_vector) # = 8.944272
#----------------------#
#Problem-11
#----------------------#
data<-read.csv("exams_and_names.csv")
View(data)
exam2mean<-mean(data[,3],na.rm=TRUE)
exam2sd<-sd(data[,3],na.rm=TRUE)
z<-(data[,3]-exam2mean)/exam2sd
sort(z)
#----------------------#
#Problem-11
#----------------------#
data<-read.csv("exams_and_names.csv")
exam1mean<-mean(data[,2],na.rm=TRUE)
exam1sd<-sd(data[,2],na.rm=TRUE)
z<-(data[,2]-exam1mean)/exam1sd
sort(z)
sorted<-sort(z)
min(sorted)
max(sorted)
#----------------------#
#Problem-12
#----------------------#
exam2mean<-mean(data[,3],na.rm=TRUE)
exam2sd<-sd(data[,3],na.rm=TRUE)
z<-(data[,3]-exam2mean)/exam2sd
sorted<-sort(z)
min(sorted) # = -2.554536
max(sorted) # = 1.280846
#----------------------#
#Problem-11
#----------------------#
data<-read.csv("spring2008exams.csv")
exam1mean<-mean(data[,2],na.rm=TRUE)
exam1sd<-sd(data[,2],na.rm=TRUE)
z<-(data[,2]-exam1mean)/exam1sd
sorted<-sort(z)
min(sorted) # = -2.554536
max(sorted) # = 1.280846
#----------------------#
#Problem-12
#----------------------#
exam2mean<-mean(data[,3],na.rm=TRUE)
exam2sd<-sd(data[,3],na.rm=TRUE)
z<-(data[,3]-exam2mean)/exam2sd
sorted<-sort(z)
min(sorted) # = -2.648063
max(sorted) # = 1.24245
#----------------------#
#Problem-14
#----------------------#
data<-read.csv("spring2008exams.csv")
q1<-quantile(data[,3],.25,na.rm=TRUE)
q3<-quantile(data[,3],.75,na.rm=TRUE)
iqr<-q3-q1
data[(data[,3]>q3+1.5*iqr),3]
data[(data[,3]<q1-1.5*iqr),3]
boxplot(data[,2],data[,3],col="blue", main="Exam Scores", names=c("Exam 1","Exam 2"),ylab="Exam Score")
#----------------------#
#Problem-15
#----------------------#
data<-read.csv("spring2008exams.csv")
model<-lm(data[,3]~data[,2])
plot(data[,2],data[,3],pch=19,xlab="Exam 1", ylab="Exam2",xlim=c(100,200),ylim=c(100,200))
abline(model)
sort(model$residuals)
#----------------------#
#Problem-15
#----------------------#
data<-read.csv("spring2008exams.csv")
model<-lm(data[,3]~data[,2])
plot(data[,2],data[,3],pch=19,xlab="Exam 1", ylab="Exam2",xlim=c(100,200),ylim=c(100,200))
View(data)
View(data)
model<-lm(data[,3]~data[,2])
plot(data[,2],data[,3],pch=19,xlab="Exam 1", ylab="Exam2",xlim=c(50,100),ylim=c(50,100))
abline(model)
sort(model$residuals)
#----------------------#
#Problem-11
#----------------------#
data<-read.csv("spring2008exams.csv")
exam1mean<-mean(data[,2],na.rm=TRUE)
exam1sd<-sd(data[,2],na.rm=TRUE)
z<-(data[,2]-exam1mean)/exam1sd
sorted<-sort(z)
min(sorted) # = -2.283753
max(sorted) # = 1.84958
#there are no outliers
sorted
--version
version
boxplot(data[,2],data[,3],col="blue", main="Exam Scores", names=c("Exam 1","Exam 2"),ylab="Exam Score")
setwd("E:/Data-Mining/Week 2/Assignment 5")
#----------------------#
#Problem-2
#----------------------#
data = read.csv("sonar_test.csv", header = FALSE)
x = data[, 1:2]
plot(x, pch = 19, xlab = expression(x[1]), ylab = expression(x[2]))
fit = kmeans(x, 2)
points(fit$centers, pch=19, col="blue", cex=2)
library(class)
knnfit<-knn(fit$centers, x, as.factor(c(-1,1)))
points(x, col=1+1*as.numeric(knnfit), pch=19)
#----------------------#
#Problem-3
#----------------------#
plot(x, pch=19, xlab=expression(x[1]), ylab=expression(x[2]))
y<-data[,61]
points(x, col=2+1*y, pch=19)
#misclassification error
1-sum(knnfit==y)/length(y) #0.525641
#----------------------#
#Problem-4
#----------------------#
x<-data[,1:60]
fit<-kmeans(x, 2)
knnfit<-knn(fit$centers, x, as.factor(c(-1,1)))
1-sum(knnfit==y)/length(y) #0.4358
#----------------------#
#Problem-6
#----------------------#
x<-c(1,2,2.5,3,3.5,4,4.5,5,7,8,8.5,9,9.5,10)
center1<-1
center2<-2
for (k in 2:10){
cluster1<-x[abs(x-center1[k-1])<=abs(x-center2[k-1])]
cluster2<-x[abs(x-center1[k-1])>abs(x-center2[k-1])]
center1[k]<-mean(cluster1)
center2[k]<-mean(cluster2)
}
#----------------------#
#Problem-7
#----------------------#
x<-c(1,2,2.5,3,3.5,4,4.5,5,7,8,8.5,9,9.5,10)
kmeans(x,2)
#----------------------#
#Problem-8
#----------------------#
x1<-c(1,2)
x2<-c(5,10)
#(b)
dist_vector = rbind(x1, x2)
dist(dist_vector) # = 8.944272
#----------------------#
#Problem-9
#----------------------#
x1<-c(1,2,3,6)
x2<-c(5,10,4,12)
#(b)
dist_vector = rbind(x1, x2)
dist(dist_vector) # = 10.81665
#----------------------#
#Problem-11
#----------------------#
data<-read.csv("spring2008exams.csv")
exam1mean<-mean(data[,2],na.rm=TRUE)
exam1sd<-sd(data[,2],na.rm=TRUE)
z<-(data[,2]-exam1mean)/exam1sd
sorted<-sort(z)
min(sorted) # = -2.283753
max(sorted) # = 1.84958
#----------------------#
#Problem-12
#----------------------#
exam2mean<-mean(data[,3],na.rm=TRUE)
exam2sd<-sd(data[,3],na.rm=TRUE)
z<-(data[,3]-exam2mean)/exam2sd
sorted<-sort(z)
min(sorted) # = -2.396223
max(sorted) # = 1.299726
#----------------------#
#Problem-14
#----------------------#
data<-read.csv("spring2008exams.csv")
q1<-quantile(data[,3],.25,na.rm=TRUE)
q3<-quantile(data[,3],.75,na.rm=TRUE)
iqr<-q3-q1
data[(data[,3]>q3+1.5*iqr),3] #no outlier
data[(data[,3]<q1-1.5*iqr),3] # 64 is outlier
boxplot(data[,2],data[,3],col="blue", main="Exam Scores", names=c("Exam 1","Exam 2"),ylab="Exam Score")
#----------------------#
#Problem-15
#----------------------#
data<-read.csv("spring2008exams.csv")
model<-lm(data[,3]~data[,2])
plot(data[,2],data[,3],pch=19,xlab="Exam 1", ylab="Exam2",xlim=c(50,100),ylim=c(50,100))
abline(model)
sort(model$residuals)
setwd("E:/Data-Mining/Week 2/Assignment 4")
#----------------------#
#Problem-5
#----------------------#
install.packages("rpart")
library(rpart)
train<-read.csv("sonar_train.csv",header=FALSE)
y<-as.factor(train[,61])
x<-train[,1:60]
fit<-rpart(y~.,x,control=rpart.control(minsplit=0,minbucket=0,cp=-1, maxcompete=0, maxsurrogate=0, usesurrogate=0, xval=0,maxdepth=5))
#plotting of decission tree
plot(fit)
text(fit)
#Accuracy for training set
sum(y==predict(fit,x,type="class"))/length(y)
#miscalassification error rate for training set
1-sum(y==predict(fit,x,type="class"))/length(y)
#misclassification error rate for testing set
test<-read.csv("sonar_test.csv",header=FALSE)
x_test<-test[,1:60]
y_test<-as.factor(test[,61])
1-sum(y_test==predict(fit,x_test,type="class"))/length(y_test)
#----------------------#
#Problem-7
#----------------------#
install.packages("randomForest")
library(randomForest)
train<-read.csv("sonar_train.csv",header=FALSE)
y<-as.factor(train[,61])
x<-train[,1:60]
fit<-randomForest(x,y)
1-sum(y==predict(fit,x))/length(y)
#----------------------#
#Problem-8
#----------------------#
#(a)
#for the k-nearest neighbor classifier for k=5 and k=6.
install.packages("class")
library(class)
#misclasification error on training set
train<-read.csv("sonar_train.csv",header=FALSE)
y<-as.factor(train[,61])
x<-train[,1:60]
#k = 5
fit1<-knn(x,x,y,k=5)
#misclassification error on training data at k = 5 and k = 6
1-sum(y==fit1)/length(y)
fit2<-knn(x,x,y,k=6)
1-sum(y==fit2)/length(y)
#misclassification error on testing data at k = 5 and k = 6
test<-read.csv("sonar_test.csv",header=FALSE)
y_test<-as.factor(test[,61])
x_test<-test[,1:60]
#k = 5
fit_test<-knn(x,x_test,y,k=5)
1-sum(y_test==fit_test)/length(y_test)
#k = 6
fit_test2<-knn(x,x_test,y,k=6)
1-sum(y_test==fit_test2)/length(y_test)
#(b)
#on testing data
#misclassification rate at k = 6
fit_test2<-knn(x,x_test,y,k=6)
1-sum(y_test==fit_test2)/length(y_test)
#misclassification rate at k = 5
fit_test<-knn(x,x_test,y,k=5)
1-sum(y_test==fit_test)/length(y_test)
setwd("E:/Data-Mining/Week 2/Assignment 4")
#----------------------#
#Problem-5
#----------------------#
install.packages("rpart")
library(rpart)
train<-read.csv("sonar_train.csv",header=FALSE)
y<-as.factor(train[,61])
x<-train[,1:60]
fit<-rpart(y~.,x,control=rpart.control(minsplit=0,minbucket=0,cp=-1, maxcompete=0, maxsurrogate=0, usesurrogate=0, xval=0,maxdepth=5))
#plotting of decission tree
plot(fit)
text(fit)
#Accuracy for training set
sum(y==predict(fit,x,type="class"))/length(y)
#miscalassification error rate for training set
1-sum(y==predict(fit,x,type="class"))/length(y)
#misclassification error rate for testing set
test<-read.csv("sonar_test.csv",header=FALSE)
x_test<-test[,1:60]
y_test<-as.factor(test[,61])
1-sum(y_test==predict(fit,x_test,type="class"))/length(y_test)
#----------------------#
#Problem-7
#----------------------#
install.packages("randomForest")
library(randomForest)
train<-read.csv("sonar_train.csv",header=FALSE)
y<-as.factor(train[,61])
x<-train[,1:60]
fit<-randomForest(x,y)
1-sum(y==predict(fit,x))/length(y)
#----------------------#
#Problem-8
#----------------------#
#(a)
#for the k-nearest neighbor classifier for k=5 and k=6.
install.packages("class")
library(class)
#misclasification error on training set at k = 5 and k = 6
train<-read.csv("sonar_train.csv",header=FALSE)
y<-as.factor(train[,61])
x<-train[,1:60]
#k = 5
fit1<-knn(x,x,y,k=5)
1-sum(y==fit1)/length(y)
#k=6
fit2<-knn(x,x,y,k=6)
1-sum(y==fit2)/length(y)
#misclassification error on testing data at k = 5 and k = 6
test<-read.csv("sonar_test.csv",header=FALSE)
y_test<-as.factor(test[,61])
x_test<-test[,1:60]
#k = 5
fit_test<-knn(x,x_test,y,k=5)
1-sum(y_test==fit_test)/length(y_test)
#k = 6
fit_test2<-knn(x,x_test,y,k=6)
1-sum(y_test==fit_test2)/length(y_test)
#(b)
#on testing data
#misclassification rate at k = 6
fit_test2<-knn(x,x_test,y,k=6)
1-sum(y_test==fit_test2)/length(y_test)
#misclassification rate at k = 5
fit_test<-knn(x,x_test,y,k=5)
1-sum(y_test==fit_test)/length(y_test)
#In general, we can choose any value for 'k',i.e, either even or odd.
#In general, we can choose any value for 'k',i.e, either even or odd.
#But it is better to choose an odd value for k when there are 2 classes.
#In general, we can choose any value for 'k',i.e, either even or odd.
#But it is better to choose an odd value for k when there are 2 classes.
#This is because, if k value is even there might be a risk of a tie in the decision of which class to choose.
#In general, we can choose any value for 'k',i.e, either even or odd.
#But it is better to choose an odd value for k when there are 2 classes.
#This is because, if k value is even there might be a risk of a tie in the decision of which class to choose.
#And because of this tie, the misclassification error rate often changes for k=6.
#In general, we can choose any value for 'k',i.e, either even or odd.
#But it is better to choose an odd value for k when there are 2 classes.
#This is because, if k value is even there might be a risk of a tie in the decision of which class to choose.
#And because of this tie, the misclassification error rate often changes for k=6.
#In general, we can choose any value for 'k',i.e, either even or odd.
#But it is better to choose an odd value for k when there are 2 classes.
#This is because, if k value is even there might be a risk of a tie in the decision of which class to choose.
#And because of this tie, the misclassification error rate often changes for k=6.
#In general, we can choose any value for 'k',i.e, either even or odd.
#But it is better to choose an odd value for k when there are 2 classes.
#This is because, if k value is even there might be a risk of a tie in the decision of which class to choose.
#And because of this tie, the misclassification error rate often changes for k=6.
#In general, we can choose any value for 'k',i.e, either even or odd.
#But it is better to choose an odd value for k when there are 2 classes.
#This is because, if k value is even there might be a risk of a tie in the decision of which class to choose.
#And because of this tie, the misclassification error rate often changes for k=6.
#In general, we can choose any value for 'k',i.e, either even or odd.
#But it is better to choose an odd value for k when there are 2 classes.
#This is because, if k value is even there might be a risk of a tie in the decision of which class to choose.
#And because of this tie, the misclassification error rate often changes for k=6.
#In general, we can choose any value for 'k',i.e, either even or odd.
#But it is better to choose an odd value for k when there are 2 classes.
#This is because, if k value is even there might be a risk of a tie in the decision of which class to choose.
#And because of this tie, the misclassification error rate often changes for k=6.
#In general, we can choose any value for 'k',i.e, either even or odd.
#But it is better to choose an odd value for k when there are 2 classes.
#This is because, if k value is even there might be a risk of a tie in the decision of which class to choose.
#And because of this tie, the misclassification error rate often changes for k=6.
#In general, we can choose any value for 'k',i.e, either even or odd.
#But it is better to choose an odd value for k when there are 2 classes.
#This is because, if k value is even there might be a risk of a tie in the decision of which class to choose.
#And because of this tie, the misclassification error rate often changes for k=6.
